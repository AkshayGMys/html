#RNN
import tensorflow as tf
from tensorflow.keras.datasets import imdb
from tensorflow.keras.preprocessing.sequence import pad_sequences
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix, accuracy_score

# Load and preprocess dataset
(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=20000)
x_train, x_test = pad_sequences(x_train, maxlen=100), pad_sequences(x_test, maxlen=100)

# Build and compile model
model = tf.keras.Sequential([
    tf.keras.layers.Embedding(input_dim=20000, output_dim=128, input_length=100),
    tf.keras.layers.LSTM(units=128, activation='tanh'),
    tf.keras.layers.Dense(units=1, activation='sigmoid')
])
model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])

# Train model
history = model.fit(x_train, y_train, batch_size=128, epochs=5, validation_data=(x_test, y_test))

# Predictions and evaluation
y_pred = (model.predict(x_test) > 0.5).astype("int32")
print(confusion_matrix(y_test, y_pred))
print(accuracy_score(y_test, y_pred))

# Plot learning curve
epochs = range(1, 6)
plt.plot(epochs, history.history['accuracy'], label='Train')
plt.plot(epochs, history.history['val_accuracy'], label='Validation')
plt.title('Model Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

plt.plot(epochs, history.history['loss'], label='Train')
plt.plot(epochs, history.history['val_loss'], label='Validation')
plt.title('Model Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.show()







#CNN
import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix, accuracy_score
from tensorflow.keras.datasets import mnist

# Load and preprocess dataset
(x_train, y_train), (x_test, y_test) = mnist.load_data()
x_train, x_test = x_train / 255.0, x_test / 255.0
x_train = x_train.reshape(-1, 28, 28, 1)
x_test = x_test.reshape(-1, 28, 28, 1)

# Build and compile model
model = tf.keras.Sequential([
    tf.keras.Input(shape=(28, 28, 1)),  # Explicit Input layer
    tf.keras.layers.Conv2D(32, (3,3), activation='relu'),
    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),
    tf.keras.layers.MaxPool2D((2,2)),
    tf.keras.layers.Dropout(0.4),
    tf.keras.layers.Flatten(),
    tf.keras.layers.Dense(128, activation='relu'),
    tf.keras.layers.Dense(10, activation='softmax')
])
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['sparse_categorical_accuracy'])

# Train model
history = model.fit(x_train, y_train, batch_size=128, epochs=10, validation_data=(x_test, y_test))

# Model predictions and evaluation
y_pred = model.predict(x_test).argmax(axis=1)
print(confusion_matrix(y_test, y_pred))
print(accuracy_score(y_test, y_pred))

# Plot learning curves
epochs = range(1, 11)
plt.plot(epochs, history.history['sparse_categorical_accuracy'], label='Train')
plt.plot(epochs, history.history['val_sparse_categorical_accuracy'], label='Validation')
plt.title('Model Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

plt.plot(epochs, history.history['loss'], label='Train')
plt.plot(epochs, history.history['val_loss'], label='Validation')
plt.title('Model Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.show()







GOOGLE PRICE PREDICTOR
import tensorflow as tf
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import MinMaxScaler

# Read datasets from local directory
train_data = pd.read_csv(r"C:\Users\Admin\Desktop\Google_Stock_Price_Train.csv")
test_data = pd.read_csv(r"C:\Users\Admin\Desktop\Google_Stock_Price_Test.csv")

# Preprocess training data
train_set = train_data.iloc[:, 1:2].values
sc = MinMaxScaler(feature_range=(0, 1))
train_set_scaled = sc.fit_transform(train_set)

# Create training sequences
x_train, y_train = [], []
for i in range(60, len(train_set_scaled)):
    x_train.append(train_set_scaled[i-60:i, 0])
    y_train.append(train_set_scaled[i, 0])
x_train, y_train = np.array(x_train), np.array(y_train)
x_train = x_train.reshape(x_train.shape[0], x_train.shape[1], 1)

# Build LSTM model
model = tf.keras.models.Sequential([
    tf.keras.layers.LSTM(60, activation='relu', return_sequences=True,),
    tf.keras.layers.Dropout(0.2),
    tf.keras.layers.LSTM(60, activation='relu', return_sequences=True),
    tf.keras.layers.Dropout(0.2),
    tf.keras.layers.LSTM(80, activation='relu', return_sequences=True),
    tf.keras.layers.Dropout(0.2),
    tf.keras.layers.LSTM(120, activation='relu'),
    tf.keras.layers.Dropout(0.2),
    tf.keras.layers.Dense(1)
])
model.compile(optimizer='adam', loss='mean_squared_error')
model.fit(x_train, y_train, batch_size=32, epochs=100)

# Preprocess test data
real_stock_price = test_data.iloc[:, 1:2].values
total_data = pd.concat((train_data['Open'], test_data['Open']), axis=0)
inputs = total_data[len(total_data) - len(test_data) - 60:].values.reshape(-1, 1)
inputs = sc.transform(inputs)

# Create test sequences
x_test = np.array([inputs[i-60:i, 0] for i in range(60, 80)])
x_test = x_test.reshape(x_test.shape[0], x_test.shape[1], 1)

# Predict stock prices
predicted_stock_price = model.predict(x_test)
predicted_stock_price = sc.inverse_transform(predicted_stock_price)

# Visualization
plt.plot(real_stock_price, color='red', label='Real Google Stock Price')
plt.plot(predicted_stock_price, color='blue', label='Predicted Google Stock Price')
plt.title('Google Stock Price Prediction')
plt.xlabel('Time')

plt.ylabel('Google Stock Price')
plt.legend()
plt.show()







KMEANS
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans

data = list(zip([4, 5, 10, 4, 3, 11, 14, 6, 10, 12], [21, 19, 24, 17, 16, 25, 24, 22, 21, 21]))
inertias = [KMeans(n_clusters=i).fit(data).inertia_ for i in range(1, 11)]

plt.plot(range(1, 11), inertias)
plt.title('Elbow method')
plt.xlabel('Number of clusters')
plt.ylabel('Inertia')
plt.show()

kmeans = KMeans(n_clusters=2).fit(data)
plt.scatter(*zip(*data), c=kmeans.labels_)
plt.show()





BACKPROPAGATION


import numpy as np
# Define the input data and target output
X = np.array(([2, 9], [1, 5], [3, 6]), dtype=float)
y = np.array(([92], [86], [89]), dtype=float) 
X = X / np.amax(X, axis=0)
y = y / 100

# Activation function (sigmoid) and its derivative
def sigmoid(x):
    return 1 / (1 + np.exp(-x))

def derivatives_sigmoid(x):
    return x * (1 - x)

# Neural network parameters
epoch = 5 
lr = 0.1 
inputlayer_neurons = 2 
hiddenlayer_neurons = 3 
output_neurons = 1 

# Weight and bias initialization
wh = np.random.uniform(size=(inputlayer_neurons, hiddenlayer_neurons)) 
bh = np.random.uniform(size=(1, hiddenlayer_neurons)) 
wout = np.random.uniform(size=(hiddenlayer_neurons, output_neurons)) 
bout = np.random.uniform(size=(1, output_neurons)) 

# Training loop
for i in range(epoch):
    # Forward Propagation
    hinp = np.dot(X, wh) + bh
    hlayer_act = sigmoid(hinp)
    outinp = np.dot(hlayer_act, wout) + bout
    output = sigmoid(outinp)
    
    # Backpropagation
    EO = y - output 
    outgrad = derivatives_sigmoid(output)
    d_output = EO * outgrad
    EH = d_output.dot(wout.T)
    hiddengrad = derivatives_sigmoid(hlayer_act)
    d_hiddenlayer = EH * hiddengrad
    
    # Update weights and biases
    wout += hlayer_act.T.dot(d_output) * lr   
    wh += X.T.dot(d_hiddenlayer) * lr
    
    # Print epoch results
    print(f"-----------Epoch-{i+1} Starts----------")
    print("Input:\n", X)
    print("Actual Output:\n", y)
    print("Predicted Output:\n", output)
    print(f"-----------Epoch-{i+1} Ends----------\n")

# Final prediction after training
print("Input:\n", X)
print("Actual Output:\n", y)
print("Predicted Output:\n", output)
